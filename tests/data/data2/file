FIRST
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
About the instructor: Tim Roughgarden has been a professor in the Computer Science Department at Stanford University since 2004.  He has taught and published extensively on the subject of algorithms and their applications.

The A* Search algorithm (pronounced “A star”) is an alternative to the Dijkstra’s Shortest Path algorithm. It is used to find the shortest path between two nodes of a weighted graph. The A* Search algorithm performs better than the Dijkstra’s algorithm because of its use of heuristics.

Before investigating this algorithm make sure you are familiar with the terminology used when describing Graphs in Computer Science.

Let’s decompose the A* Search algorithm step by step using the example provided below. (Use the tabs below to progress step by step).

Note that, in this graph, the heuristic we will use is the straight line distance (“as the crow flies”) between a node and the end node (Z). This distance will always be the shortest distance between two points, a distance that cannot be reduced whatever path you follow to reach node Z.

A basic requirement is that the function should provide a uniform distribution of hash values. A non-uniform distribution increases the number of collisions and the cost of resolving them. Uniformity is sometimes difficult to ensure by design, but may be evaluated empirically using statistical tests, e.g., a Pearson's chi-squared test for discrete uniform distributions.[6][7]

The distribution needs to be uniform only for table sizes that occur in the application. In particular, if one uses dynamic resizing with exact doubling and halving of the table size, then the hash function needs to be uniform only when the size is a power of two. Here the index can be computed as some range of bits of the hash function. On the other hand, some hashing algorithms prefer to have the size be a prime number.[8] The modulus operation may provide some additional mixing; this is especially useful with a poor hash function.

For open addressing schemes, the hash function should also avoid clustering, the mapping of two or more keys to consecutive slots. Such clustering may cause the lookup cost to skyrocket, even if the load factor is low and collisions are infrequent. The popular multiplicative hash[3] is claimed to have particularly poor clustering behavior.[8]

Cryptographic hash functions are believed to provide good hash functions for any table size, either by modulo reduction or by bit masking[citation needed]. They may also be appropriate if there is a risk of malicious users trying to sabotage a network service by submitting requests designed to generate a large number of collisions in the server's hash tables. However, the risk of sabotage can also be avoided by cheaper methods (such as applying a secret salt to the data, or using a universal hash function). A drawback of cryptographic hashing functions is that they are often slower to compute, which means that in cases where the uniformity for any size is not necessary, a non-cryptographic hashing function might be preferable.[citation needed]

As the load factor grows larger, the hash table becomes slower, and it may even fail to work (depending on the method used). The expected constant time property of a hash table assumes that the load factor be kept below some bound. For a fixed number of buckets, the time for a lookup grows with the number of entries, and therefore the desired constant time is not achieved. In some implementations, the solution is to automatically grow (usually, double) the size of the table when the load factor bound is reached, thus forcing to re-hash all entries. As a real-world example, the default load factor for a HashMap in Java 10 is 0.75, which "offers a good trade-off between time and space costs."[9]

Second to the load factor, one can examine the variance of number of entries per bucket. For example, two tables both have 1,000 entries and 1,000 buckets; one has exactly one entry in each bucket, the other has all entries in the same bucket. Clearly the hashing is not working in the second one.

A low load factor is not especially beneficial. As the load factor approaches 0, the proportion of unused areas in the hash table increases, but there is not necessarily any reduction in search cost. This results in wasted memory.
Main article: Great Books of the Western World
The Great Books of the Western World is a hardcover 60-volume collection (originally 54 volumes) of the books on the Great Books list (517 individual works). A prominent feature of the collection is a two-volume Syntopicon (meaning "a collection of topics") that includes essays written by Mortimer Adler on 102 "great ideas." Following each essay is an extensive outline of the idea with page references to relevant passages throughout the collection. The collection was available from Encyclopædia Britannica, Inc., which owns the copyright.

In contemporary scholarship, the Great Books curriculum was drawn into the popular debate about multiculturalism, traditional education, the "culture war," and the role of the intellectual in American life.

Many had issue with the lack of culture added to this list in both the first and second editions due to its lack of diversity in ethnic origin, as many Hispanic- and African-American documents were overlooked because they did not meet the ideals of drawing from the Great Ideas, of which the chosen texts had to meet a total of 25 at the least, to be considered as Great Books.

But this only seems to say persons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.
ersons and communities make choices about how they wish to participate intellectually in tradition in which they inhabit. Howard University, the first historically-black college had only been open 65 years when the last work for the 1952 set had been written. Further, Great books curriculum extends much further than the selection set of one, although one formerly-famous, series of books, and the Great Ideas were not really canonical; rather, the books were, the Great Ideas being invented as concept clusters to identify broad areas of common study down through the ages, though in very many cases slow to conclude, if having yet at all, and often with interruptions, hesitations and reversals.

Much of this debate centered on reactions to the publication of The Closing of the American Mind in 1987 by Allan Bloom.[66] In his book, Allan Bloom suggested that the shortcoming of teaching by the method of the Great books is that it focuses primarily on historical reading without allowing for the point of view of the reader in this day and age.

He argued that this limited the ability for our knowledge to grow, given that no perspective was given in regards to the advancement of civilization past the date of these books. But although the 1952 set ended with a work from 1932, the 1990 second edition had elements designed to meet those objections, such as the six 20th-century-selection volumes, as large, if not larger, than a typical book from the set.

One of the purposes, however, of the set is to help a reader to fundamental mastery of idea topics, which while time-intensive and requiring study beyond a liberal-arts college degree, is apt to lead to the reader making their own decisions of how or whether modern works extend or transcend ideas understood by way of the intellectual traditions preceding those concept clusters.

Shortly after Adler retired from the Great Books Foundation in 1989, he served as editor-in-chief in publishing that second edition (1990) of the Great Books of the Western World for Encyclopædia Britannica; it included more Hispanic and female authors and, for the first time, works by black authors.[67] During his tenure as president of the Foundation, Adler had resisted such additions.[68] Ultimately, he remarked:

We did not base our selections on an author's nationality, religion, politics, or field of study; nor on an author's race or gender. Great books were not chosen to make up quotas of any kind; there was no "affirmative action" in the process ... we chose the Great books on the basis of their relevance to at least 25 of the 102 great ideas. Many of the Great books are relevant to a much larger number of the 102 great ideas, as many as 75 or more great ideas, a few to all 102 great ideas. In sharp contrast are the good books that are relevant to less than 10 or even as few as 4 or 5 great ideas. We placed such books in the lists of Recommended Readings to be found in the last section in each of the 102 chapters of the "Syntopicon". Here readers will find many twentieth-century female authors, black authors, and Latin American authors whose works we recommended but did not include in the second edition of the great books [as main entries].[3]

[Robert Hutchins wrote in 1951,] "In the course of history ... new books have been written that have won their place in the list. Books once thought entitled to belong to it have been superseded; and this process of change will continue as long as men can think and write. It is the task of every generation to reassess the tradition in which it lives, to discard what it cannot use, and to bring into context with the distant and intermediate past the most recent contributions to the Great Conversation."[69]

Before and after the 1990 20th-century-authors volumes were included, additions to the Great Books main entries were also published from 1953 to 2002 in the Great Ideas Today yearbook series volumes (matching the styling of the set like updates to encyclopedias are). The first work from the modern world (some of the races of authors in the ancient world are unknown) chosen for inclusion in these additions to the Great Books main entries by a black author was The Souls of Black Folk by W. E. B. Dubois (1903).
